{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de Decisión, Proyecto Corto 3 Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlace de Github: https://github.com/brayanfa07/IA-Proyecto-Corto-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brayan Fajardo Alvarado\n",
    "- Fabricio Castillo Alvarado\n",
    "- Gerald Mora Mora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto\n",
    "\n",
    "\n",
    "Un árbol de decisión es una forma gráfica y analítica de representar todos los eventos surgidos a partir de una decisión asumida en cualquier momento. Estos árboles ayudan a tomar una decisión más acertada desde un punto de vista probabilístico ante una gran cantidad de posibles decisiones.\n",
    "## Características de los árboles de decisión\n",
    "\n",
    "-    Plantea el problema desde distintas perspectivas de acción.\n",
    "-    Permite analizar de manera completa todas las posibles soluciones.\n",
    "-    Ayuda a realizar las mejores decisiones con base a la información existente y a las mejores suposiciones.\n",
    "-    Su estructura permite analizar las alternativas, los eventos, las probabilidades y los resultados.\n",
    "\n",
    "## Como construir un árbol de decisión\n",
    "### Se define el problema\n",
    "\n",
    "Se debe analizar el caso en estudio y considerar si amerita la implementación de un árbol de decisión o de regresión.\n",
    "### Se identifican todas las variables o atributos del problema a resolver\n",
    "\n",
    "Una vez que se identifique cual es el problema a resolver, se debe reconocer cuales son los factores que lo componen.\n",
    "### Se enumeran los criterios a tomar\n",
    "\n",
    "Es importante priorizar cuales son los factores que componen el problema, para ello se deben seleccionar cuales son los que más influyen en el problema, así tener el problema limitado a los factores de mayor relevancia.\n",
    "### Se dibuja el árbol de decisión\n",
    "\n",
    "Se debe considerar la terminología de nodo de decisión, nodo de probabilidad y rama-\n",
    "\n",
    "- Nodo de decisión: Indica que una decisión necesita tomarse en ese punto del proceso. Está representado por un cuadrado.\n",
    "- Nodo de probabilidad: Indica que en ese punto del proceso ocurre un evento aleatorio. Está representado por un círculo.\n",
    "- Rama: Nos muestra los distintos caminos que se pueden emprender cuando tomamos una decisión o bien ocurre algún evento aleatorio.\n",
    "\n",
    "### Se selecciona una alternativa\n",
    "\n",
    "Se analiza cuál es la opción más conveniente de acuerdo al árbol de decisiones, siempre se toma en cuenta la importancia de los criterios y cada una de sus alternativas. Luego de ello se estima los resultados para cada combinación de posible de alternativas.\n",
    "### Se evalúa la efectividad de la decisión\n",
    "\n",
    "Se resuelve el problema obteniendo como solución de la ruta que proporcione la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como desarrollar un arbol de decision en python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan las librerias necesarias para el funcionamiento del codigo.\n",
    "Se carga el fichero de datos llamado zoo.csv y se indican las variables predictoras omitiendo la primer columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como desarrollar un arbol de decision en python\n",
    "Se cargan las librerias necesarias para el funcionamiento del codigo.\n",
    "Se carga el fichero de datos llamado zoo.csv y se indican las variables predictoras omitiendo la primer columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "#Import the dataset and define the feature as well as the target datasets / columns#\n",
    "dataset = pd.read_csv('zoo.csv',\n",
    "                      names=['animal_name','hair','feathers','eggs','milk',\n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals\n",
    "#We drop the animal names since this is not a good feature to split the data on\n",
    "dataset=dataset.drop('animal_name',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion entropy calcula la entropia del set de datos.\n",
    "\n",
    "**Entrada:** \n",
    "- target, este parametro representa la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "\n",
    "    \n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo para la ganancia\n",
    "La funcion InfoGain obtiene la ganacia del set de datos.\n",
    "\n",
    "**Entradas:** \n",
    "- data, set de datos.\n",
    "\n",
    "- split_attribute_name, el nombre del atributo cuyo valor de ganancia va a ser calculada.\n",
    "\n",
    "- tarjet_name, target, este parametro representa la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
    "\n",
    "    \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo arbol ID3\n",
    "Esta funcion representa el algoritmo de arbol de decision ID3 visto en la **Figura 1**. \n",
    "https://fotos.subefotos.com/402c311ac839f43335aee53154159cf4o.png <center>**Figura 1.** El algoritmo de aprendizaje del árbol de decisión. [1]</center>\n",
    "\n",
    "**Entradas:** \n",
    "- data, esta es la informacion sobre la cual el algoritmo va a actuar.\n",
    "\n",
    "- originaldata, este es el conjunto de datos original necesario para calcular el valor de la \n",
    "característica de destino de modo del conjunto de datos original.\n",
    "\n",
    "- features, es el espacio de atributos del conjunto de datos, el cual es necesario para la \n",
    "llamada recursiva, ya que durante el proceso de crecimiento del árbol.\n",
    "\n",
    "- target_attribute_name,es el nombre de la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legs': {0: {'fins': {0.0: {'toothed': {0.0: 7.0, 1.0: 3.0}},\n",
      "                       1.0: {'eggs': {0.0: 1.0, 1.0: 4.0}}}},\n",
      "          2: {'hair': {0.0: 2.0, 1.0: 1.0}},\n",
      "          4: {'hair': {0.0: {'toothed': {0.0: 7.0, 1.0: 5.0}}, 1.0: 1.0}},\n",
      "          6: {'aquatic': {0.0: 6.0, 1.0: 7.0}},\n",
      "          8: 7.0}}\n",
      "La exactitud de la precisión es:  85.71428571428571 %\n"
     ]
    }
   ],
   "source": [
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n",
    "\n",
    "    #Define los criterios de condiciones de parada de ejecución del algoritmo. Si uno de estos criterios se cumple, \n",
    "    #se debe retornar el nodo hoja\n",
    "    \n",
    "    #Si todos los valores objetivos poseen el mismo valor, se devuelve el mismo valor\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #Si el conjunto de datos está vacío, se devuelve el valor objetivo del conjunto de datos original\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "    #Si la característica es vacía, devuelve el valor objetivo del valor del atributo del nodo padre.\n",
    "    #Note que el nodo directo del padre el cual se llama desde la ejecución actual del algoritmo ID3, y por lo tanto, \n",
    "    #el valor objetivo del atributo  es almacenado en la clase del nodo padre\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #Si el nodo de abajo se mantiene \"verdadero\", se continúa creciendo el árbol\n",
    "    else:\n",
    "\n",
    "        #Define el valor por defecto para este nodo, es decir, el valor objetivo del nodo actual\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #Selecciona el atributo con la mejor división para el conjunto de datos\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Devuelve la ganancia de la informaión para los atributos\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        \n",
    "        #Crea la estructura del árbol. Se escoge la raíz como el atributo con la mejor ganancia de información y \n",
    "        #se inserta en la raíz\n",
    "        tree = {best_feature:{}}\n",
    "              \n",
    "        #Remover el atributo con la mayor ganancia desde el espacio de atributos\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Expandir una rama debajo del nodo raíz por cada posible valor de la los atributos de la raíz\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "\n",
    "            #Divide el conjunto de datos a lo largo del valor de la característica con el mayor valor de\n",
    "            #ganancia de información, y con esto crea otros subconjuntos\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Llama al algoritmo ID3 para cada uno de los subconjuntos con los nuevos parámetros. Se inicia con la recursióm\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Añade el subárbol, crecido desde el subconjunto de datos hasta el nodo debajo del nodo raíz\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "    \n",
    "def predict(query,tree,default = 1):\n",
    "    #Se consulta en la lista de los tributos a consultar en el árbol.\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "    \n",
    "    \n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "  \n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "            else:\n",
    "                return result\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Se comprueba la exactitud de la predicción. La función train_test_split toma el conjunto de datos como \n",
    "parámetros, la cual divide los datos para entrenamiento y para prueba, que se ingresan al modelo de árbol\n",
    "\"\"\"\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    \n",
    "    #Se elimina el atributo inicial o el ubicado en la posición 0, para que no se produzccan errores debido a que se leen \n",
    "    #strings o cadenas de caracteres.\n",
    "    training_data = dataset.iloc[:80].reset_index(drop=True)\n",
    "    \n",
    "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
    "    return training_data,testing_data\n",
    "\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] \n",
    "\n",
    "def test(data,tree):\n",
    "\n",
    "    #Crea una nueva instancia de consulta removiendo el target de la columna \n",
    "    #de atributos desde el conjunto de datos original y lo convierte en un diccionario\n",
    "    \n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Crea un Dataframe vacío en el cual se guardan las columnas de predicción\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    \n",
    "    #Calcula la precisión de predicción\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    print('La exactitud de la precisión es: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Se entrena y se imprime el árbol, y además se predice la exactitud\n",
    "\"\"\"\n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "pprint(tree)\n",
    "test(testing_data,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena y se imprime el árbol, y además se predice la exactitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una técnica que combina una cantidad grande de arboles de decisión independientes probados sobre conjuntos de datos aleatorios con igual distribución.\n",
    "## ¿Como construir un Random Forest?\n",
    "\n",
    "Para el *proceso de aprendizaje* se empieza con la creación de varios arboles de decisión independientes, para los cuales cada uno tendrá un conjunto de datos ligeramente diferenciados. Osea que solamente se altera el set de datos iniciales.\n",
    "Por lo tanto se debe considerar:\n",
    "- Seleccionar aleatoria-mente un porcentaje de datos de la muestra total.\n",
    "- En cada nodo, al seleccionar una partición optima, se tendrá en cuenta una sola partición de los atributos que se eligen al azar en cada ocasión.\n",
    "\n",
    "Para el *proceso de clasificación* se llevara a cabo de la siguiente forma:\n",
    "- Cada árbol se avaluara de forma independiente y la predicción sera la media de todos los arboles que se estén utilizando. La proporción de arboles que toman una misma respuesta se interpreta como la probabilidad de la misma.\n",
    "\n",
    "``` python\n",
    "\n",
    "# Lo primero que hay que hacer es incluir las librerias necesarias para realizar la ejecucion.\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Cargamos el fichero de datos para utilizar, el cual es un csv en este caso. archivo.csv contiene los datos necesarios para la creacion del arbol.\n",
    "AH_data = pd.read_csv(“archivo.csv”)\n",
    "\n",
    "# Se eliminan los datos con valores missing ya que Python presenta errores al hacer árboles con datos missing\n",
    "data_clean = AH_data.dropna()\n",
    "\n",
    "# Para comprobar que el archivo se ha leído bien, se puede listar las variables en el fichero\n",
    "data_clean.dtypes\n",
    "# Principales estadísticos\n",
    "data_clean.describe()\n",
    "\n",
    "#Se indican las variables predictoras y debajo la variable objetivo. Cada uno con los nombres de las variables que se tienen en el fichero csv. En este caso se intercambiaria VARPRED1, VARPRED2, VARPRED3 por los nombres de las variables del fichero.\n",
    "predictors = data_clean[[‘VARPRED1’, ‘VARPRED2′,’VARPRED3’]] \n",
    "\n",
    "targets = data_clean.TREG1\n",
    "\n",
    "# Se crea la muestra de entrenamiento y de test, tanto para predictores como para la variable objetivo, siendo test el 40%\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, targets, test_size=.4)\n",
    "\n",
    "# Se inicializa el algoritmo Random Forest y se indica el número de árboles que se van a construir\n",
    "classifier = RandomForestClassifier(n_estimators=25)\n",
    "\n",
    "# Se construye el modelo sobre los datos de entrenamiento creados anteriormente\n",
    "classifier = classifier.fit(pred_train,tar_train)\n",
    "\n",
    "# Se realiza una prediccion para los valores del grupo Test\n",
    "predictions=classifier.predict(pred_test)\n",
    "\n",
    "# Se establece la matriz de confusión de las predicciones del grupo Test.\n",
    "sklearn.metrics.confusion_matrix(tar_test,predictions)\n",
    "\n",
    "# Se calcula el índice Accuracy Score, que resume la Matriz de Confusión y la cantidad de aciertos.\n",
    "sklearn.metrics.accuracy_score(tar_test, predictions)\n",
    "\n",
    "# Para obtener la importancia de cada variable se inicializa el ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "# Se ajusta el modelo\n",
    "model.fit(pred_train,tar_train)\n",
    "\n",
    "# Se puede solicitar y visualizar la importancia de cada variable\n",
    "print(model.feature_importances_)\n",
    "\n",
    "# En caso de que se tengan muchas variables, se pueben visualizar con el comando “list”\n",
    "list(model.feature_importances_)\n",
    "\n",
    "# Para dibujar todas las variables con su importancia respectiva\n",
    "pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "pyplot.show()\n",
    "\n",
    "# Para ver cuánto ha aportado cada nuevo árbol que se ha construido\n",
    "trees=range(25)\n",
    "accuracy=np.zeros(25)\n",
    "for idx in range(len(trees)):\n",
    "classifier=RandomForestClassifier(n_estimators=idx + 1)\n",
    "classifier=classifier.fit(pred_train,tar_train)\n",
    "predictions=classifier.predict(pred_test)\n",
    "accuracy[idx]=sklearn.metrics.accuracy_score(tar_test, predictions)\n",
    "plt.cla()\n",
    "plt.plot(trees, accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.python-course.eu/Decision_Trees.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
